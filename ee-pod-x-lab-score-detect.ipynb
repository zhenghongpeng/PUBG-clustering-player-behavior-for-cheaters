{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card fraud detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution we will build the core of a credit card fraud detection system using SageMaker. We will start by \n",
    "\n",
    "\n",
    "ing an anomaly detection algorithm, then proceed to train two XGBoost models for supervised training. To deal with the highly unbalanced data common in fraud detection, our first model will use re-weighting of the data, and the second will use re-sampling, using the popular SMOTE technique for oversampling the rare fraud data.\n",
    "\n",
    "Our solution includes an example of making calls to a REST API to simulate a real deployment, using AWS Lambda to trigger both the anomaly detection and XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the credit card fraud data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/PUBG-clustering-player-behavior-for-cheaters\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r /home/ec2-user/SageMaker/PUBG-clustering-player-behavior-for-cheaters/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp s3://sagemaker-fraud-machine-learning-inputbucket-ee-x-pod-lab-kp /home/ec2-user/SageMaker/source/notebooks --recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 ls s3://sagemaker-fraud-machine-learning-inputbucket-ee-x-pod-lab-kp/pubgclustering/data/PUBG_Player_Statistics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/PUBG-clustering-player-behavior-for-cheaters\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import boto\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('creditcard.csv', delimiter=',')\n",
    "file = \"/home/ec2-user/SageMaker/PUBG-clustering-player-behavior-for-cheaters/data/PUBG_Player_Statistics.csv\"\n",
    "data = pd.read_csv(file, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our data (we only show a subset of the columns in the table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['player_name', 'tracker_id', 'solo_KillDeathRatio', 'solo_WinRatio',\n",
      "       'solo_TimeSurvived', 'solo_RoundsPlayed', 'solo_Wins',\n",
      "       'solo_WinTop10Ratio', 'solo_Top10s', 'solo_Top10Ratio',\n",
      "       ...\n",
      "       'squad_RideDistance', 'squad_MoveDistance', 'squad_AvgWalkDistance',\n",
      "       'squad_AvgRideDistance', 'squad_LongestKill', 'squad_Heals',\n",
      "       'squad_Revives', 'squad_Boosts', 'squad_DamageDealt', 'squad_DBNOs'],\n",
      "      dtype='object', length=152)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "# data[['Time', 'V1', 'V2', 'V27', 'V28', 'Amount', 'Class']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Preprocessing\n",
    "## Create a copy of the dataframe\n",
    "df = data.copy()\n",
    "cols = np.arange(52, 152, 1)\n",
    "\n",
    "# Drop entries if they have null values\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "## Drop columns after the 52nd index\n",
    "df.drop(df.columns[cols], axis = 1, inplace = True)\n",
    "\n",
    "## Drop player_name and tracker id\n",
    "df.drop(df.columns[[0, 1]], axis = 1, inplace = True)\n",
    "\n",
    "## Drop Knockout and Revives\n",
    "df.drop(df.columns[[49]], axis = 1, inplace = True)\n",
    "df.drop(columns = ['solo_Revives'], inplace = True)\n",
    "\n",
    "## Drop the string solo from all strings\n",
    "df.rename(columns = lambda x: x.lstrip('solo_').rstrip(''), inplace = True)\n",
    "\n",
    "## Combine a few columns \n",
    "df['TotalDistance'] = df['WalkDistance'] + df['RideDistance']\n",
    "df['AvgTotalDistance'] = df['AvgWalkDistance'] + df['AvgRideDistance']\n",
    "\n",
    "# Remove Outliers\n",
    "df = df.drop(df[df['RoundsPlayed'] < df['RoundsPlayed'].mean()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['KillDeathRatio', 'WinRatio', 'TimeSurvived', 'RoundsPlayed', 'Wins',\n",
       "       'WinTop10Ratio', 'Top10s', 'Top10Ratio', 'Losses', 'Rating',\n",
       "       'BestRating', 'DamagePg', 'HeadshotKillsPg', 'HealsPg', 'KillsPg',\n",
       "       'MoveDistancePg', 'RevivesPg', 'RoadKillsPg', 'TeamKillsPg',\n",
       "       'TimeSurvivedPg', 'Top10sPg', 'Kills', 'Assists', 'Suicides',\n",
       "       'TeamKills', 'HeadshotKills', 'HeadshotKillRatio', 'VehicleDestroys',\n",
       "       'RoadKills', 'DailyKills', 'WeeklyKills', 'RoundMostKills',\n",
       "       'MaxKillStreaks', 'WeaponAcquired', 'Days', 'LongestTimeSurvived',\n",
       "       'MostSurvivalTime', 'AvgSurvivalTime', 'WinPoints', 'WalkDistance',\n",
       "       'RideDistance', 'MoveDistance', 'AvgWalkDistance', 'AvgRideDistance',\n",
       "       'LongestKill', 'Heals', 'Boosts', 'DamageDealt', 'TotalDistance',\n",
       "       'AvgTotalDistance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples is 20771\n",
      "The number of development samples is 7121\n",
      "The number of testing samples is 1781\n"
     ]
    }
   ],
   "source": [
    "# Create train and test set using Sci-Kit Learn\n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 10)\n",
    "dev, test = train_test_split(test, test_size = 0.2, random_state = 10)\n",
    "data = train\n",
    "\n",
    "print(\"The number of training samples is\", len(train))\n",
    "print(\"The number of development samples is\", len(dev))\n",
    "print(\"The number of testing samples is\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       KillDeathRatio      WinRatio  TimeSurvived  RoundsPlayed          Wins  \\\n",
      "count    20771.000000  20771.000000  2.077100e+04  20771.000000  20771.000000   \n",
      "mean         1.289158      2.204012  1.484172e+05    174.985894      3.554475   \n",
      "std          0.602602      2.510500  9.339460e+04    113.147056      4.939222   \n",
      "min          0.100000      0.000000  3.813548e+04     80.000000      0.000000   \n",
      "25%          0.900000      0.680000  9.091498e+04    104.000000      1.000000   \n",
      "50%          1.160000      1.460000  1.195404e+05    139.000000      2.000000   \n",
      "75%          1.520000      2.910000  1.733681e+05    205.000000      4.000000   \n",
      "max         17.410000     40.210000  1.219536e+06   1552.000000    102.000000   \n",
      "\n",
      "       WinTop10Ratio        Top10s    Top10Ratio        Losses        Rating  \\\n",
      "count   20771.000000  20771.000000  20771.000000  20771.000000  20771.000000   \n",
      "mean        0.138708     23.884743     14.369067    171.431419   2059.159131   \n",
      "std         0.137145     19.214653      7.396966    111.734872    256.747029   \n",
      "min         0.000000      1.000000      0.700000     58.000000   1165.510000   \n",
      "25%         0.040000     13.000000      9.300000    101.000000   1882.635000   \n",
      "50%         0.100000     19.000000     12.800000    136.000000   2074.280000   \n",
      "75%         0.200000     28.000000     17.600000    201.000000   2240.980000   \n",
      "max         1.000000    386.000000     69.300000   1541.000000   2967.090000   \n",
      "\n",
      "         BestRating      DamagePg  HeadshotKillsPg       HealsPg  \\\n",
      "count  20771.000000  20771.000000     20771.000000  20771.000000   \n",
      "mean    2084.405850    152.375769         0.269829      1.387851   \n",
      "std      248.783836     58.102445         0.171563      0.613690   \n",
      "min     1279.220000     15.590000         0.010000      0.070000   \n",
      "25%     1909.505000    113.610000         0.170000      1.020000   \n",
      "50%     2100.270000    141.620000         0.230000      1.280000   \n",
      "75%     2257.935000    178.490000         0.330000      1.620000   \n",
      "max     2963.690000   1054.120000         5.220000     15.720000   \n",
      "\n",
      "            KillsPg  MoveDistancePg  RevivesPg   RoadKillsPg   TeamKillsPg  \\\n",
      "count  20771.000000    20771.000000    20771.0  20771.000000  20771.000000   \n",
      "mean       1.249489     2564.498896        0.0      0.018677      0.007097   \n",
      "std        0.534357      914.865530        0.0      0.019422      0.008446   \n",
      "min        0.100000      603.150000        0.0      0.000000      0.000000   \n",
      "25%        0.890000     1939.505000        0.0      0.010000      0.000000   \n",
      "50%        1.140000     2393.870000        0.0      0.010000      0.010000   \n",
      "75%        1.480000     2988.270000        0.0      0.030000      0.010000   \n",
      "max       10.410000    14527.380000        0.0      0.350000      0.160000   \n",
      "\n",
      "       TimeSurvivedPg      Top10sPg         Kills       Assists      Suicides  \\\n",
      "count    20771.000000  20771.000000  20771.000000  20771.000000  20771.000000   \n",
      "mean       866.861773      0.143653    212.706129     13.568533      1.189302   \n",
      "std        165.230571      0.074049    169.973630     10.984160      1.520992   \n",
      "min        384.860000      0.010000     20.000000      0.000000      0.000000   \n",
      "25%        751.730000      0.090000    119.000000      7.000000      0.000000   \n",
      "50%        855.710000      0.130000    166.000000     11.000000      1.000000   \n",
      "75%        966.225000      0.180000    247.000000     16.000000      2.000000   \n",
      "max       1669.600000      0.690000   4023.000000    185.000000     67.000000   \n",
      "\n",
      "          TeamKills  HeadshotKills  HeadshotKillRatio  VehicleDestroys  \\\n",
      "count  20771.000000   20771.000000       20771.000000     20771.000000   \n",
      "mean       1.189351      45.870011           0.208889         1.556930   \n",
      "std        1.521128      43.602108           0.053425         2.605163   \n",
      "min        0.000000       2.000000           0.040000         0.000000   \n",
      "25%        0.000000      23.000000           0.170000         0.000000   \n",
      "50%        1.000000      34.000000           0.210000         1.000000   \n",
      "75%        2.000000      54.000000           0.240000         2.000000   \n",
      "max       67.000000    1494.000000           0.760000       138.000000   \n",
      "\n",
      "          RoadKills    DailyKills   WeeklyKills  RoundMostKills  \\\n",
      "count  20771.000000  20771.000000  20771.000000    20771.000000   \n",
      "mean       3.249242      8.490925     21.468538        8.319002   \n",
      "std        4.340528     10.347829     27.086182        2.941070   \n",
      "min        0.000000      0.000000      0.000000        2.000000   \n",
      "25%        1.000000      2.000000      6.000000        6.000000   \n",
      "50%        2.000000      5.000000     13.000000        8.000000   \n",
      "75%        4.000000     11.000000     27.000000       10.000000   \n",
      "max      171.000000    174.000000    491.000000       87.000000   \n",
      "\n",
      "       MaxKillStreaks  WeaponAcquired          Days  LongestTimeSurvived  \\\n",
      "count    20771.000000         20771.0  20771.000000         20771.000000   \n",
      "mean         2.468634             0.0     33.988012          2025.299524   \n",
      "std          1.347912             0.0     12.724328           103.533922   \n",
      "min          1.000000             0.0      4.000000          1612.800000   \n",
      "25%          2.000000             0.0     25.000000          1960.120000   \n",
      "50%          2.000000             0.0     32.000000          1991.500000   \n",
      "75%          3.000000             0.0     40.000000          2100.150000   \n",
      "max         87.000000             0.0    173.000000          3067.930000   \n",
      "\n",
      "       MostSurvivalTime  AvgSurvivalTime     WinPoints  WalkDistance  \\\n",
      "count      20771.000000     20771.000000  20771.000000  2.077100e+04   \n",
      "mean        2025.299524       906.716177   3085.571037  2.183727e+05   \n",
      "std          103.533922       210.977929   1536.193318  1.403832e+05   \n",
      "min         1612.800000       242.940000    959.000000  4.848569e+04   \n",
      "25%         1960.120000       761.985000   1803.000000  1.339626e+05   \n",
      "50%         1991.500000       888.930000   2764.000000  1.767646e+05   \n",
      "75%         2100.150000      1038.490000   3952.000000  2.553466e+05   \n",
      "max         3067.930000      1797.320000  10143.000000  2.457887e+06   \n",
      "\n",
      "       RideDistance  MoveDistance  AvgWalkDistance  AvgRideDistance  \\\n",
      "count  2.077100e+04  2.077100e+04     20771.000000     20771.000000   \n",
      "mean   2.230056e+05  4.413784e+05      1346.563388      1354.913718   \n",
      "std    2.099482e+05  3.280358e+05       494.991381       858.724381   \n",
      "min    5.001280e+03  6.091792e+04       236.830000        30.810000   \n",
      "25%    1.051215e+05  2.486792e+05      1068.300000       745.990000   \n",
      "50%    1.637230e+05  3.458146e+05      1293.540000      1157.880000   \n",
      "75%    2.662136e+05  5.169522e+05      1558.880000      1744.530000   \n",
      "max    3.453278e+06  4.592642e+06     28756.940000      9127.700000   \n",
      "\n",
      "        LongestKill         Heals        Boosts    DamageDealt  TotalDistance  \\\n",
      "count  20771.000000  20771.000000  20771.000000   20771.000000   2.077100e+04   \n",
      "mean     324.427150    243.003226    215.759087   26040.816089   4.413784e+05   \n",
      "std      118.718927    204.151779    172.743355   19913.848424   3.280358e+05   \n",
      "min       19.820000     23.000000      9.000000    3219.700000   6.091792e+04   \n",
      "25%      250.585000    129.000000    115.000000   14826.670000   2.486791e+05   \n",
      "50%      306.840000    185.000000    167.000000   20426.280000   3.458145e+05   \n",
      "75%      375.460000    284.000000    255.000000   30325.895000   5.169522e+05   \n",
      "max     4694.110000   6341.000000   2923.000000  442283.700000   4.592642e+06   \n",
      "\n",
      "       AvgTotalDistance  \n",
      "count      20771.000000  \n",
      "mean        2701.477106  \n",
      "std         1112.561617  \n",
      "min          354.890000  \n",
      "25%         1936.070000  \n",
      "50%         2506.620000  \n",
      "75%         3267.805000  \n",
      "max        30077.250000  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', 52):\n",
    "    print(data.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = train['Rating']\n",
    "feature_train = train.drop('Rating', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test = test['Rating']\n",
    "feature_test = test.drop('Rating', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data (Normalize)\n",
    "scaler1 = StandardScaler()\n",
    "X_train = scaler1.fit_transform(feature_train)\n",
    "scaler2 = StandardScaler()\n",
    "y_train = scaler2.fit_transform(label_train.values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data (Normalize)\n",
    "\n",
    "X_test = scaler1.transform(feature_test)\n",
    "y_test = scaler2.transform(label_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20771, 49)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have gathered an adequate amount of labeled training data, we can use a supervised learning algorithm that discovers relationships between the features and the dependent class.\n",
    "\n",
    "We will use Gradient Boosted Trees as our model, as they have a proven track record, are highly scalable and can deal with missing data, reducing the need to pre-process datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we copy the data to an in-memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [y[0] for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import sklearn\n",
    "from sklearn.datasets import dump_svmlight_file   \n",
    "from package import config\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "buf = io.BytesIO()\n",
    "bucket = config.MODEL_DATA_S3_BUCKET\n",
    "prefix = 'fraud-classifier-score-detection'\n",
    "session = sagemaker.Session()\n",
    "\n",
    "sklearn.datasets.dump_svmlight_file(X_train, y_train, buf)\n",
    "buf.seek(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we upload the data to S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-fraud-machine-learning-modeldatabucket-pb9mj8rs1vhd/fraud-classifier-score-detection/train/base/fraud-dataset-score1\n",
      "Training artifacts will be uploaded to: s3://sagemaker-fraud-machine-learning-modeldatabucket-pb9mj8rs1vhd/fraud-classifier-score-detection/output\n"
     ]
    }
   ],
   "source": [
    "key = 'fraud-dataset-score1'\n",
    "subdir = 'base'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', subdir, key)).upload_fileobj(buf)\n",
    "\n",
    "s3_train_data = 's3://{}/{}/train/{}/{}'.format(bucket, prefix, subdir, key)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now train using SageMaker's built-in XGBoost algorithm. To specify the XGBoost algorithm, we use a utility function to obtain its URI. A complete list of built-in algorithms is found here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='1.0-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SageMaker abstracts training via Estimators. We can pass the classifier and parameters along with hyperparameters to the estimator, and fit the estimator to the data in S3. An important parameter here is `scale_pos_weight` which scales the weights of the positive vs. negative class examples. This is crucial to do in an imbalanced dataset like the one we are using here, otherwise the majority class would dominate the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Because the data set is so highly skewed, we set the scale position weight conservatively,\n",
    "# as sqrt(num_nonfraud/num_fraud).\n",
    "# Other recommendations for the scale_pos_weight are setting it to (num_nonfraud/num_fraud).\n",
    "\n",
    "hyperparams = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\":\"reg:linear\",\n",
    "        \"num_round\":\"50\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explain the hyper-parameters used above. The one that's very relevant for learning from skewed data is `scale_pos_weight`. This is a ratio that weighs the examples of the positive class (fraud) against the negative class (legitimate). Commonly this is set to `(num_nonfraud/num_fraud)`, but our data is exteremely skewed so we will set it to `sqrt(num_nonfraud/num_fraud)`.  For the data in this example, this would be `sqrt(284,807/492)` which would give our fraud examples a weight of ~24.\n",
    "\n",
    "The rest of the hyper-parameters are as follows:\n",
    "\n",
    "* `max_depth`: This is the maximum depth of the trees that will be built for our ensemble. A max depth of 5 will give us trees with up to 32 leaves. Note that tree size grows exponentially when increasing this parameter (`num_leaves=2^max_depth`), so a max depth of 10 would give us trees with 1024 leaves, which are likely to overfit.\n",
    "* `subsample`: The subsample ratio that we use to select a subset of the complete data to train each tree in the ensemble. With a value of 0.8, each tree is trained on a random sample containing 80% of the complete data. This is used to prevent overfitting.\n",
    "* `num_round`: This is the size of the ensemble. We will for 100 \"rounds\", each training round adding a new tree to the ensemble.\n",
    "* `eta`: This is the step size shrinkage applied at each update. This value will shrink the weights of new features to prevent overfitting.\n",
    "* `gamma`: This is the minimum loss reduction to reach before splitting a leaf. Splitting a leaf can sometimes have a small benefit, and splitting such leaves can lead to overfitting. By setting `gamma` to values larger than zero, we ensure that there should be at least some non-negligible amount of accuracy gain before splitting a leaf.\n",
    "* `min_child_weight`: This parameter has a similar effect to gamma, setting it to higher values means we'll wait until enough gain will be possible before splitting a leaf.\n",
    "* `objective`: We are doing binary classification, so we use a logistic loss objective.\n",
    "* `eval_metric`: Having a good evaluation metric is crucial when dealing with imbalanced data (see discussion below). We use AUC here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "clf = sagemaker.estimator.Estimator(container,\n",
    "                                    get_execution_role(),\n",
    "                                    hyperparameters=hyperparams,\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our supervised training model, the call to fit below should take around 5 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-23 00:14:51 Starting - Starting the training job...\n",
      "2020-09-23 00:14:53 Starting - Launching requested ML instances......\n",
      "2020-09-23 00:16:14 Starting - Preparing the instances for training.........\n",
      "2020-09-23 00:17:48 Downloading - Downloading input data\n",
      "2020-09-23 00:17:48 Training - Downloading the training image...\n",
      "2020-09-23 00:18:21 Uploading - Uploading generated training model.\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:linear to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[00:18:18] 20771x49 matrix with 976237 entries loaded from /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 20771 rows\u001b[0m\n",
      "\u001b[34m[00:18:18] WARNING: /workspace/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[34m[00:18:18] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.90318\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.73254\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.59746\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.49146\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.40926\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.34607\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.29806\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.26242\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.23610\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.21644\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:0.20367\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:0.19369\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:0.18748\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:0.18235\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:0.17962\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:0.17722\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:0.17476\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:0.17371\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:0.17295\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:0.17136\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:0.17103\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:0.17070\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:0.17069\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:0.16994\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:0.16993\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:0.16993\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:0.16992\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:0.16992\u001b[0m\n",
      "\n",
      "2020-09-23 00:18:29 Completed - Training job completed\n",
      "Training seconds: 58\n",
      "Billable seconds: 58\n"
     ]
    }
   ],
   "source": [
    "clf.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Host Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we deploy the estimator to and endpoint. As before progress will be indicated by `-`, and the deployment should be done after 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = clf.deploy(initial_instance_count=1,\n",
    "                       model_name=\"{}-xgb-score2\".format(config.STACK_NAME),\n",
    "                       endpoint_name=\"{}-xgb-score2\".format(config.STACK_NAME),\n",
    "                       instance_type='ml.m4.xlarge', \n",
    "                       serializer=csv_serializer,\n",
    "                       deserializer=None,\n",
    "                       content_type='text/csv'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the model we can use it to make predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Because we have a large test set, we call predict on smaller batches\n",
    "def predict(current_predictor, data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, current_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = predict(predictor, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12294080013954453"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(raw_preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scaler2.inverse_transform(raw_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1848.66862436, 2535.32131814, 1673.89029883, 1942.82280686,\n",
       "       2652.46228703, 2008.38311133, 2082.21514075, 1990.20157275,\n",
       "       1960.61736451, 2016.94419206, 2737.14591935, 2062.73540727,\n",
       "       2229.97074948, 2499.78695679, 2275.62574551, 2121.69896321,\n",
       "       2119.75447128, 2300.04836151, 2212.26562226, 2084.63478919])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57444    1880.39\n",
       "3409     2554.83\n",
       "29238    1671.34\n",
       "5743     1960.45\n",
       "11757    2718.97\n",
       "33620    1995.55\n",
       "14746    2066.20\n",
       "43903    1987.61\n",
       "41285    1949.17\n",
       "87335    2028.89\n",
       "145      2813.79\n",
       "42213    2108.29\n",
       "2183     2216.46\n",
       "1676     2546.16\n",
       "86401    2301.43\n",
       "15801    2137.92\n",
       "61349    2132.76\n",
       "28713    2298.64\n",
       "11791    2193.26\n",
       "52764    2080.42\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
